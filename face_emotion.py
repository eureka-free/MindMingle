import cv2
import numpy as np
from deepface import DeepFace
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import av
import threading
import time

# WebRTC i√ßin gerekli yapƒ±landƒ±rma
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# Duygu tanƒ±ma i√ßin thread-safe deƒüi≈üken
class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.emotion_result = None
        self.last_detection_time = time.time()
        self.detection_interval = 1.0  # saniye cinsinden
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.lock = threading.Lock()

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        current_time = time.time()
        
        # Y√ºz tespiti ve √ßer√ßeve √ßizimi
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        
        for (x, y, w, h) in faces:
            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # Belirli aralƒ±klarla duygu analizi yap (CPU kullanƒ±mƒ±nƒ± azaltmak i√ßin)
            if current_time - self.last_detection_time >= self.detection_interval and len(faces) > 0:
                try:
                    # DeepFace ile duygu analizi
                    face_img = img[y:y+h, x:x+w]
                    result = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)
                    
                    with self.lock:
                        self.emotion_result = result[0]["emotion"]
                    
                    self.last_detection_time = current_time
                except Exception as e:
                    print(f"Duygu analizi hatasƒ±: {e}")
                    pass
            
            # Eƒüer bir duygu tespiti yapƒ±ldƒ±ysa, ekranda g√∂ster
            if self.emotion_result:
                emotion_text = max(self.emotion_result.items(), key=lambda x: x[1])[0]
                cv2.putText(img, emotion_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)
        
        return av.VideoFrame.from_ndarray(img, format="bgr24")

    def get_emotion(self):
        with self.lock:
            return self.emotion_result

def map_emotion_to_mood(emotion):
    """
    DeepFace'in tespit ettiƒüi duyguyu uygulamamƒ±zƒ±n ruh hali kategorilerine e≈üler
    """
    if emotion is None:
        return None
    
    dominant_emotion = max(emotion.items(), key=lambda x: x[1])[0]
    
    # Duygu-ruh hali e≈üle≈ütirme
    emotion_mood_map = {
        "happy": "Mutlu",
        "surprise": "√áok Mutlu",
        "neutral": "Keyifli",
        "sad": "Melankolik",
        "fear": "Melankolik",
        "angry": "√úzg√ºn",
        "disgust": "√úzg√ºn"
    }
    
    return emotion_mood_map.get(dominant_emotion, "Keyifli")

def emotion_detector_component():
    """
    Streamlit uygulamasƒ±na eklenebilecek duygu tanƒ±ma bile≈üeni
    """
    st.write("## üì∑ Y√ºz ƒ∞fadesi ile Ruh Hali Analizi")
    st.write("Kameranƒ±zƒ± a√ßarak y√ºz ifadenize g√∂re otomatik ruh hali tespiti yapabilirsiniz.")
    
    ctx = webrtc_streamer(
        key="emotion-detection",
        video_transformer_factory=VideoTransformer,
        rtc_configuration=RTC_CONFIGURATION,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )
    
    if ctx.video_transformer:
        st.write("Y√ºz√ºn√ºz√º kameraya doƒüru konumlandƒ±rƒ±n...")
        
        # Duygu sonu√ßlarƒ±nƒ± g√∂stermek i√ßin yer a√ßma
        emotion_placeholder = st.empty()
        mood_placeholder = st.empty()
        
        # Ger√ßek zamanlƒ± duygu takibi
        while ctx.state.playing:
            # Duygusal durumu alma
            emotion = ctx.video_transformer.get_emotion()
            
            if emotion:
                # Duygularƒ± g√∂rselle≈ütirme
                emotion_placeholder.write("### Tespit Edilen Duygular:")
                
                # √áubuk grafiƒüi olu≈üturma
                emotions_df = {
                    "Duygu": list(emotion.keys()),
                    "Skor": list(emotion.values())
                }
                
                # En belirgin duyguyu bulma
                dominant_emotion = max(emotion.items(), key=lambda x: x[1])[0]
                
                # Duygulara g√∂re ruh hali e≈üle≈ütirme
                mood = map_emotion_to_mood(emotion)
                mood_placeholder.success(f"### Tahmini Ruh Haliniz: {mood}")
                
                # Deƒüerler y√ºzde olarak g√∂sterme
                for emo, score in emotion.items():
                    bar_length = int(score * 100)
                    formatted_score = f"{score*100:.1f}%"
                    
                    # Belirgin duygu i√ßin farklƒ± renk
                    color = "green" if emo == dominant_emotion else "gray"
                    
                    st.write(f"{emo.capitalize()}: {formatted_score}")
                    st.progress(score)
                
                # Analizin fazla sƒ±k g√ºncellenmemesi i√ßin kƒ±sa bekleme
                time.sleep(0.5)
    
    # Tespit edilen ruh halini d√∂nd√ºr
    if ctx.video_transformer and ctx.state.playing:
        emotion = ctx.video_transformer.get_emotion()
        if emotion:
            return map_emotion_to_mood(emotion)
    
    return None
